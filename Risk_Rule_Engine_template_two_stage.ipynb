{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8febac5",
   "metadata": {},
   "source": [
    "# Risk Rule Engine — Notebook Template\n",
    "\n",
    "This notebook is the **working template** for the Risk Rule Engine project, structured as a **two-stage engine**:\n",
    "\n",
    "1. **Feature Selection Engine** (`feature_selection_py/`)  \n",
    "   Produces a *rule-ready* feature specification (`variable_spec`) and diagnostics artifacts (IV, correlation, VIF, worst-tail ranking).\n",
    "\n",
    "2. **Rule Construction Engine** (`rules_py/`)  \n",
    "   Generates candidate rules (1D / 2D / 3D), evaluates each rule with a **single scalar metric** (e.g., **G:B ratio** or **Bad Balance BR-times**), and provides deep-dive impact / overlap / visualisations.\n",
    "\n",
    "---\n",
    "\n",
    "## Folder structure assumed\n",
    "\n",
    "- `feature_selection_py/`\n",
    "  - `feature_selection_pipeline.py`  ← main entrypoint for Part 1\n",
    "  - `fs_utils.py`, `fs_iv.py`, `fs_filters.py`, `fs_plots.py`\n",
    "  - `feature_engineering.py` (optional)\n",
    "\n",
    "- `rules_py/`\n",
    "  - `rule_metrics.py`\n",
    "  - `rules_searching.py`\n",
    "  - `rule_impact_analysis.py`\n",
    "  - `rule_overlap_analysis.py`\n",
    "  - `rule_visualisation.py`\n",
    "\n",
    "_Last updated: 2026-01-24_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e9cc9",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Run this once per session. Adjust `PROJECT_ROOT` if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8458e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# If you open the notebook from repo root, this is fine:\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# If you open from a subfolder, uncomment and point to repo root:\n",
    "# PROJECT_ROOT = Path(\"/workspaces/NextGen_Rule_Engine\")\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a9154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b54a97b",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "Replace this section with your real data loading logic.\n",
    "\n",
    "Required (typical) columns:\n",
    "- `BAD_FLAG`: 1/0 indicator (bad)\n",
    "- `TOTAL_BAL`: total exposure / balance\n",
    "- `BAD_BAL`: bad exposure / balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da1ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace with your data source\n",
    "# Example:\n",
    "# data = pd.read_parquet(PROJECT_ROOT / \"data\" / \"dataset.parquet\")\n",
    "\n",
    "data = pd.DataFrame()  # placeholder\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adabaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your column names here\n",
    "BAD_FLAG = \"bad_flag\"\n",
    "TOTAL_BAL = \"written_amount\"\n",
    "BAD_BAL = \"bad_balance\"\n",
    "\n",
    "required_cols = [BAD_FLAG, TOTAL_BAL, BAD_BAL]\n",
    "missing = [c for c in required_cols if c not in data.columns]\n",
    "if missing:\n",
    "    print(\"⚠️ Missing columns:\", missing)\n",
    "else:\n",
    "    print(\"✅ Required columns present\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e04bc",
   "metadata": {},
   "source": [
    "# Part 1 — Feature Selection Engine\n",
    "\n",
    "**Goal:** produce a rule-ready feature specification (`variable_spec`) that will be consumed by the rule construction engine.\n",
    "\n",
    "Outputs:\n",
    "- `selected_num`, `selected_cat`\n",
    "- `variable_spec` (DataFrame)\n",
    "- `artifacts` (IV, correlation pairs, VIF, worst-tail ranking tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0eec79",
   "metadata": {},
   "source": [
    "## 2. Import feature selection pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f05108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selection_py.feature_selection_pipeline import (\n",
    "    run_feature_selection_pipeline,\n",
    "    FeatureSelectionConfig,\n",
    ")\n",
    "\n",
    "print(\"Feature selection imports OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64399833",
   "metadata": {},
   "source": [
    "## 3. (Optional) Load data dictionary\n",
    "\n",
    "If you have a data dictionary (variable/definition/direction), load it here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7236a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "# data_dictionary = pd.read_csv(PROJECT_ROOT / \"data\" / \"data_dictionary.csv\")\n",
    "\n",
    "data_dictionary = None  # or a DataFrame\n",
    "data_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e73fe",
   "metadata": {},
   "source": [
    "## 4. Run feature selection pipeline\n",
    "\n",
    "Tune thresholds in `FeatureSelectionConfig` depending on your dataset and business need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098387e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = FeatureSelectionConfig(\n",
    "    iv_threshold=0.02,\n",
    "    corr_threshold=0.85,\n",
    "    use_vif=False,             # set True if you want VIF pruning\n",
    "    run_worst_tail_rank=True,\n",
    "    worst_pct=0.05,\n",
    "    max_features=None,         # set an int to cap\n",
    ")\n",
    "\n",
    "if len(data) > 0 and not missing:\n",
    "    fs_out = run_feature_selection_pipeline(\n",
    "        data,\n",
    "        bad_flag=BAD_FLAG,\n",
    "        bal_variable=TOTAL_BAL,\n",
    "        bad_bal_variable=BAD_BAL,\n",
    "        data_dictionary=data_dictionary,\n",
    "        config=cfg,\n",
    "    )\n",
    "\n",
    "    selected_num = fs_out[\"selected_num\"]\n",
    "    selected_cat = fs_out[\"selected_cat\"]\n",
    "    variable_spec = fs_out[\"variable_spec\"]\n",
    "    fs_artifacts = fs_out[\"artifacts\"]\n",
    "\n",
    "    print(\"Selected numeric:\", len(selected_num))\n",
    "    print(\"Selected categorical:\", len(selected_cat))\n",
    "    variable_spec.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a1081e",
   "metadata": {},
   "source": [
    "## 5. Inspect feature selection artifacts (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e235fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"fs_artifacts\" in globals():\n",
    "    fs_artifacts.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV table\n",
    "if \"fs_artifacts\" in globals() and fs_artifacts.get(\"iv_table\") is not None:\n",
    "    fs_artifacts[\"iv_table\"].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst-tail ranking (bad volume)\n",
    "if \"fs_artifacts\" in globals() and fs_artifacts.get(\"worst_tail_bad_vol\") is not None:\n",
    "    fs_artifacts[\"worst_tail_bad_vol\"].head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474b79a",
   "metadata": {},
   "source": [
    "## 6. Export `variable_spec` for rule construction (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba82a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(PROJECT_ROOT) / \"results\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "if \"variable_spec\" in globals():\n",
    "    out_path = OUT_DIR / \"variable_spec.csv\"\n",
    "    variable_spec.to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e9cca7",
   "metadata": {},
   "source": [
    "# Part 2 — Rule Construction Engine\n",
    "\n",
    "**Goal:** generate candidate rules (1D / 2D / 3D), score each rule with a **single metric**, and produce ranked tables.\n",
    "\n",
    "Metrics:\n",
    "- `G_to_B` (G:B ratio)\n",
    "- `BR_Bal_Times` (bad balance BR-times, depending on your metric implementation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f776a54",
   "metadata": {},
   "source": [
    "## 7. Import rule engine modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2538ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rules_py.rule_metrics import (\n",
    "    compute_baseline_stats,\n",
    "    rule_metric_summary,\n",
    "    combine_checking_gb_ratio,\n",
    "    combine_checking_bal_br_times,\n",
    ")\n",
    "\n",
    "from rules_py.rules_searching import (\n",
    "    Rules_Optimisation_Search_Algorithm_1D,\n",
    "    Rules_Optimisation_Search_Algorithm_2D,\n",
    "    Rules_Optimisation_Search_Algorithm_3D,\n",
    ")\n",
    "\n",
    "from rules_py.rule_impact_analysis import (\n",
    "    new_baseline_performance_after_rule,\n",
    "    build_new_baseline_table_from_rule_list,\n",
    "    # If you implemented your multi-rule impact-table builder, import it too:\n",
    "    # build_rule_impact_table_from_masks,\n",
    ")\n",
    "\n",
    "from rules_py.rule_overlap_analysis import (\n",
    "    two_rules_redundancy,\n",
    "    three_rules_redundancy,\n",
    ")\n",
    "\n",
    "from rules_py.rule_visualisation import (\n",
    "    group_performance_one_rule,\n",
    "    group_performance_two_rules,\n",
    ")\n",
    "\n",
    "print(\"Rule engine imports OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befbb517",
   "metadata": {},
   "source": [
    "## 8. Baseline stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada8d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data) > 0 and not missing:\n",
    "    baseline_stats = compute_baseline_stats(data, BAD_FLAG, TOTAL_BAL, BAD_BAL)\n",
    "    baseline_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a08a303",
   "metadata": {},
   "source": [
    "## 9. Candidate rule search (1D / 2D / 3D)\n",
    "\n",
    "Uses `variable_spec` from Part 1 as the input spec for rule construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metric_name = \"G_to_B\"   # or \"BR_Bal_Times\"\n",
    "min_bads = 10\n",
    "\n",
    "if \"variable_spec\" in globals() and len(data) > 0 and not missing and len(variable_spec) > 0:\n",
    "    df_1d = Rules_Optimisation_Search_Algorithm_1D(\n",
    "        data=data,\n",
    "        variable_dateframe=variable_spec,\n",
    "        bad_flag=BAD_FLAG,\n",
    "        bad_bal=BAD_BAL,\n",
    "        total_bal=TOTAL_BAL,\n",
    "        selected_function=None,\n",
    "        Metric_name=Metric_name,\n",
    "        min_bads=min_bads,\n",
    "    )\n",
    "    df_1d.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33340d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"variable_spec\" in globals() and len(data) > 0 and not missing and len(variable_spec) > 0:\n",
    "    df_2d = Rules_Optimisation_Search_Algorithm_2D(\n",
    "        data=data,\n",
    "        variable_dateframe=variable_spec,\n",
    "        bad_flag=BAD_FLAG,\n",
    "        bad_bal=BAD_BAL,\n",
    "        total_bal=TOTAL_BAL,\n",
    "        selected_function=None,\n",
    "        Metric_name=Metric_name,\n",
    "        min_bads=min_bads,\n",
    "    )\n",
    "    df_2d.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c357c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"variable_spec\" in globals() and len(data) > 0 and not missing and len(variable_spec) > 0:\n",
    "    df_3d = Rules_Optimisation_Search_Algorithm_3D(\n",
    "        data=data,\n",
    "        variable_dateframe=variable_spec,\n",
    "        bad_flag=BAD_FLAG,\n",
    "        bad_bal=BAD_BAL,\n",
    "        total_bal=TOTAL_BAL,\n",
    "        selected_function=None,\n",
    "        Metric_name=Metric_name,\n",
    "        min_bads=min_bads,\n",
    "        include_mixed_3d=True,\n",
    "    )\n",
    "    df_3d.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a98c7",
   "metadata": {},
   "source": [
    "## 10. Deep-dive impact for selected rules\n",
    "\n",
    "Evaluate the **new baseline** after implementing a chosen rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: deep dive on a single manual rule mask (replace with your chosen mask)\n",
    "# rule = (data[\"x\"] > 5)\n",
    "# impact = new_baseline_performance_after_rule(data, rule, BAD_FLAG, TOTAL_BAL, BAD_BAL)\n",
    "# impact\n",
    "\n",
    "print(\"TODO: select rule mask and run impact analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f9db0",
   "metadata": {},
   "source": [
    "## 11. Export ranked tables (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path(PROJECT_ROOT) / \"results\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Uncomment when tables exist\n",
    "# if \"df_1d\" in globals(): df_1d.to_csv(OUT_DIR / \"ranked_rules_1d.csv\", index=False)\n",
    "# if \"df_2d\" in globals(): df_2d.to_csv(OUT_DIR / \"ranked_rules_2d.csv\", index=False)\n",
    "# if \"df_3d\" in globals(): df_3d.to_csv(OUT_DIR / \"ranked_rules_3d.csv\", index=False)\n",
    "\n",
    "print(\"Exports folder:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
